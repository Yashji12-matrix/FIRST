{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashji12-matrix/FIRST/blob/main/NLP/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bijgqBg-VMJA"
      },
      "source": [
        "# Text Processing\n",
        "\n",
        "This notebook contains the practical examples and exercises for the Applied AI-Natural Language Processing.\n",
        "\n",
        "*Created by Hansi Hettiarachchi*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation\n",
        "Tokenisation is the task of cutting a string into identifiable linguistic units that constitute a piece of language data.\n",
        "\n",
        "Let's see how to use [tokenizers](https://www.nltk.org/api/nltk.tokenize.html) available with NLTK (Natural Language Toolkit) package to tokenise text."
      ],
      "metadata": {
        "id": "0yTL8Gxi6jrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # NLTK module required for Tokenizers\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByV9qPvw16Kc",
        "outputId": "565a1a4e-e2e0-41c8-de70-5a7a3434eb75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF5HAhIugmwP"
      },
      "source": [
        "sample_text = \"This is a sentence, which contains all kind of words, and needs to be tokenized!\"\n",
        "sample_tweet1 = \"This is a cooool :-) :-P <3 #cool\"\n",
        "sample_tweet2 = \"@remy: This is waaaaayyyy too much for you!!!!!!\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj8A08V7h1ir"
      },
      "source": [
        "Tokenising normal text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Dcb7ITfT2y",
        "outputId": "52add236-1ea9-4747-9856-2e1698303562"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "tokenized_text = word_tokenize(sample_text)\n",
        "print(tokenized_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sentence', ',', 'which', 'contains', 'all', 'kind', 'of', 'words', ',', 'and', 'needs', 'to', 'be', 'tokenized', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PlF7OQh73p"
      },
      "source": [
        "Tokenising tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAtt6c63VOU5",
        "outputId": "c49ca1eb-f4ac-4cd3-e71b-085a5b341d81"
      },
      "source": [
        "tokenized_tweet1 = word_tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = word_tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':', '-', ')', ':', '-P', '<', '3', '#', 'cool']\n",
            "tokenized tweet2: ['@', 'remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3MnOMdrh-da"
      },
      "source": [
        "As you can see in the above outputs, <i>word_tokenize</i> cannot tokenize the tweet text correctly.\n",
        "Considering the differences in tweet text compared to normal text, there is an another tokenizer named <i>TweetTokenizer</i> available with NLTK which is specifically designed for tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8LHQrQrfllV",
        "outputId": "ce05a987-8281-471b-f247-fd331c694cda"
      },
      "source": [
        "tknzr = TweetTokenizer()\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized tweet1: ['This', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'This', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-KNtZ6LjSvA"
      },
      "source": [
        "Let's analyse more features available with [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html?highlight=tweettokenizer#nltk.tokenize.casual.TweetTokenizer).\n",
        "- preserve_case (default setting=True) - Keep case sensitivity of the text\n",
        "- reduce_len (default setting=False) - Normalize text by removing repeated character sequences of length 3 or greater with sequences of length 3.\n",
        "- strip_handles (default setting=False) - Remove Twitter usernames in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv2rAXcHXqON",
        "outputId": "41378edb-02c5-457d-8e32-a8b72eeb873c"
      },
      "source": [
        "# setting1: make the tokens case insensitive or convert into lowercase\n",
        "print('configs: preserve_case=False')\n",
        "tknzr = TweetTokenizer(preserve_case=False)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs: preserve_case=False\n",
            "tokenized tweet1: ['this', 'is', 'a', 'cooool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaaaayyyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting2: make the tokens case insensitive and reduce length\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True')\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz8vi2Bni3Hy",
        "outputId": "5d526fa5-76e6-4690-aecc-afcf4187af9a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "configs: preserve_case=False, reduce_len=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: ['@remy', ':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting3: make the tokens case insensitive, reduce length and remove usernames\n",
        "print('\\nconfigs: preserve_case=False, reduce_len=True, strip_handles=True')\n",
        "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "\n",
        "tokenized_tweet1 = tknzr.tokenize(sample_tweet1)\n",
        "print(f'tokenized tweet1: {tokenized_tweet1}')\n",
        "\n",
        "tokenized_tweet2 = tknzr.tokenize(sample_tweet2)\n",
        "print(f'tokenized tweet2: {tokenized_tweet2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOZWIpAmi4z5",
        "outputId": "146aed92-a6f4-482a-ef35-ba5bd7db1c87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "configs: preserve_case=False, reduce_len=True, strip_handles=True\n",
            "tokenized tweet1: ['this', 'is', 'a', 'coool', ':-)', ':-P', '<3', '#cool']\n",
            "tokenized tweet2: [':', 'this', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'>**Activity 1**</font>\n",
        "\n",
        "Analyse the outputs from TweetTokenizer above under settings 1, 2 and 3 and identify the best setting to use, if your final aim is to predict the sentiment (positive, negative and neutral) of the tweet."
      ],
      "metadata": {
        "id": "WGHoIRPamkPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting 3 because the emotion is expressed in 3 and length is small. though username is not showing any emotion regarding to the context of this question. We can still remove few punctuation marks\n"
      ],
      "metadata": {
        "id": "LujQLjxsmSqs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qkRlhEoH6IC"
      },
      "source": [
        "# Text Normalisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oTfsvegJn5C"
      },
      "source": [
        "## Lower casing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "6MlkHXNtowNl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"The striped BATs are hanging on their feet for best\""
      ],
      "metadata": {
        "id": "vRRSaEGpnBmg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1V46nFRRcv",
        "outputId": "c901678f-1493-4ed9-860f-05c566400537"
      },
      "source": [
        "# Any string can be lower cased using the function lower()\n",
        "lower_cased_text = sample_text.lower()\n",
        "print(lower_cased_text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the striped bats are hanging on their feet for best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with string methods, you can find a list of all of them in the [documentation](https://docs.python.org/3.7/library/stdtypes.html#string-methods)."
      ],
      "metadata": {
        "id": "yrVsQP4qi-SX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9M2IprIg2H"
      },
      "source": [
        "## Stemming\n",
        "Stemming chops off the end or beginning of words by taking into account a list of common prefixes or suffixes that could be found in that word.\n",
        "\n",
        "The most common and effecive algorithm for stemming English is <i>Porter’s algorithm.</i>\n",
        "\n",
        "Details of different stemmers available with NLTK is available [here](https://www.nltk.org/howto/stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "-jY9nY_ql7Yi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vIsy8ZcIftF",
        "outputId": "22bf6004-9cc2-4793-8b69-a5e63f8031a2"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "word = \"dogs\"\n",
        "stem_word = ps.stem(word)\n",
        "\n",
        "print(f'Stemmed word: {stem_word}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have a list of words, you need to iteratively go through each to do the conversion\n",
        "sample_words = [\"dogs\", \"ponies\", \"eating\", \"corpora\"]\n",
        "stem_words = [ps.stem(word) for word in sample_words]\n",
        "\n",
        "print(f'Stemmed words: {stem_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlK4G4ZEkuK4",
        "outputId": "f667d249-37ac-42d8-d222-2743b1be8807"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['dog', 'poni', 'eat', 'corpora']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemmers take a single word as the input. If you have a sentence, you need to first tokenise it."
      ],
      "metadata": {
        "id": "oJKtinMskC3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "tokens = word_tokenize(sample_sentence)\n",
        "stem_words = [ps.stem(word) for word in tokens]\n",
        "\n",
        "print(f'Stemmed words: {stem_words}')"
      ],
      "metadata": {
        "id": "e_nqLwB2pqY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773ebc68-d90b-4a3d-9747-8406a9316380"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If required, you can also convert the stem words into a sentence by merging them with a space between each word.\n",
        "print(f'Stemmed sentence: {\" \".join(stem_words)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnzjcnC4l3oI",
        "outputId": "96829c77-5015-4c7f-9ba8-9660f637c7db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed sentence: the stripe bat are hang on their feet for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsDEnGi2I4GT"
      },
      "source": [
        "## Lemmatisation\n",
        "\n",
        "Lemmatisation is an more organised procedure to obtain the base form of a word (lemma) with the use of a vocabulary and morphological analysis (word structure and grammar relations) of words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.WordNetLemmatizer.lemmatize)"
      ],
      "metadata": {
        "id": "hQx5xW5NkKWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')  # NLTK module required for WordNetLemmatizer\n",
        "nltk.download('omw-1.4') # NLTK module required for WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8BhOdEgq53x",
        "outputId": "846c8631-70b1-4623-8738-7d2657e40048"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZTVWOvH-5m",
        "outputId": "2d5e14a9-8f0a-4c72-ccde-4d4bdc653953"
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "word = \"dogs\"\n",
        "lemma_word = wnl.lemmatize(word)\n",
        "\n",
        "print(f'Lemmatised word: {lemma_word}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised word: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have a list of words, you need to iteratively go through each to do the conversion"
      ],
      "metadata": {
        "id": "OJK7yMGmxO-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"dogs\", \"ponies\", \"eating\", \"corpora\"]\n",
        "lemma_words = [wnl.lemmatize(word) for word in sample_words]\n",
        "\n",
        "print(f'Lemmatised words: {lemma_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvHh-_nJpz4E",
        "outputId": "d71c800c-e068-4f57-a617-9326472ebae1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['dog', 'pony', 'eating', 'corpus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to Stemmmers, Lemmatizers also take a single word as the input. If you have a sentence, you need to first tokenise it."
      ],
      "metadata": {
        "id": "7U-SzO2Tp_lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "tokens = word_tokenize(sample_sentence)\n",
        "lemma_words = [wnl.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(f'Lemmatised words: {lemma_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDVPN8yip8dK",
        "outputId": "720fb06d-c034-453f-c35d-2d98c89addb2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If required, you can also convert the stem words into a sentence by merging them with a space between each word.\n",
        "print(f'Lemmatised sentence: {\" \".join(lemma_words)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDcw4N1lqiqJ",
        "outputId": "cc982de5-3b20-4578-c3d5-b567f06e1d17"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised sentence: The striped bat are hanging on their foot for best .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy Lemmatization\n",
        "\n",
        "spaCy models are pipelines designed with multiple components.<br>\n",
        "You can find more details about available pipelines and models [here](https://spacy.io/models/en).\n"
      ],
      "metadata": {
        "id": "_Q5JYM-X_KDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm  # spacy model\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "VBUHMc2Bsvoa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"The striped bats are hanging on their feet for best.\"\n",
        "\n",
        "# process a sentence using the spaCy pipeline\n",
        "doc = nlp(sample_sentence)\n",
        "# iterate through each token in the output document (processed sentence) and get its lemmatised version\n",
        "print([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GgHln8__2sU",
        "outputId": "c957e200-25bf-46fc-b669-eca65606cc10"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFMZJc5oKZ_8"
      },
      "source": [
        "### <font color='green'>**Activity 2**</font>\n",
        "\n",
        "Fill the following table by applying WordNet and spaCy lemmatization to each given word.\n",
        "\n",
        "|Original word | Lemmatised word- WordNetLemmatizer  | Lemmatised word- spaCy |\n",
        "|------|--------------------|------------|\n",
        "|walking    | ? | ? |\n",
        "|is    | ? | ? |\n",
        "|main    | ? | ? |\n",
        "|animals    | ? | ? |\n",
        "|terrestrial    | ? | ? |\n",
        "|jumping    | ? | ? |\n",
        "|best    | ? | ? |\n",
        "|sleeping    | ? | ? |\n",
        "\n",
        "Which lemmatiser is the best to normalise text and why?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"walking\", \"is\", \"main\", \"animals\", \"terrestrial\", \"jumping\", \"best\", \"sleeping\"]\n",
        "lemma_words = [wnl.lemmatize(word) for word in sample_words]\n",
        "\n",
        "print(f'Lemmatised words: {lemma_words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkTUMO7MrzhW",
        "outputId": "e9c08b5e-71ec-468c-a4c2-12a25fc3ecaa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatised words: ['walking', 'is', 'main', 'animal', 'terrestrial', 'jumping', 'best', 'sleeping']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = [\"walking\", \"is\", \"main\", \"animals\", \"terrestrial\", \"jumping\", \"best\", \"sleeping\"]\n",
        "\n",
        "# process a sentence using the spaCy pipeline\n",
        "doc = nlp(\" \".join(sample_words))\n",
        "# iterate through each token in the output document (processed sentence) and get its lemmatised version\n",
        "print([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6S_NXqDtuvu",
        "outputId": "700e101c-2bba-422c-f1fe-7538b87eb86f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['walk', 'be', 'main', 'animal', 'terrestrial', 'jumping', 'well', 'sleep']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b32a4a"
      },
      "source": [
        "### Lemmatization Results Table (Markdown Format)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "e45f5416",
        "outputId": "d2037b90-fb57-416e-88ed-7612108b3f7a"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet # Ensure wordnet is imported for wordnet.ADJ, etc.\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the specific resource requested by the error\n",
        "\n",
        "# Helper function for WordNetLemmatizer (copied from cell 1TsatnICrt8c to ensure definition)\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "words_to_lemmatize = [\"walking\", \"is\", \"main\", \"animals\", \"terrestrial\", \"jumping\", \"best\", \"sleeping\"]\n",
        "\n",
        "# WordNet Lemmatization\n",
        "wordnet_lemmas_output = []\n",
        "for word in words_to_lemmatize:\n",
        "    pos = get_wordnet_pos(word) # Get the PoS tag for WordNetLemmatizer\n",
        "    lemma = wnl.lemmatize(word, pos)\n",
        "    wordnet_lemmas_output.append(lemma)\n",
        "\n",
        "# spaCy Lemmatization\n",
        "spacy_lemmas_output = []\n",
        "# Process the entire list as a sentence for spaCy for better context awareness\n",
        "doc = nlp(\" \".join(words_to_lemmatize))\n",
        "for token in doc:\n",
        "    spacy_lemmas_output.append(token.lemma_)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "data = {\n",
        "    'Original word': words_to_lemmatize,\n",
        "    'Lemmatised word - WordNetLemmatizer': wordnet_lemmas_output,\n",
        "    'Lemmatised word - spaCy': spacy_lemmas_output\n",
        "}\n",
        "df_lemmas = pd.DataFrame(data)\n",
        "\n",
        "display(df_lemmas)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Original word Lemmatised word - WordNetLemmatizer Lemmatised word - spaCy\n",
              "0       walking                                walk                    walk\n",
              "1            is                                  be                      be\n",
              "2          main                                main                    main\n",
              "3       animals                              animal                  animal\n",
              "4   terrestrial                         terrestrial             terrestrial\n",
              "5       jumping                             jumping                 jumping\n",
              "6          best                                best                    well\n",
              "7      sleeping                               sleep                   sleep"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a58aa76-480c-4ab2-87b5-ad73fad653cc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original word</th>\n",
              "      <th>Lemmatised word - WordNetLemmatizer</th>\n",
              "      <th>Lemmatised word - spaCy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>walking</td>\n",
              "      <td>walk</td>\n",
              "      <td>walk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>be</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>animals</td>\n",
              "      <td>animal</td>\n",
              "      <td>animal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>terrestrial</td>\n",
              "      <td>terrestrial</td>\n",
              "      <td>terrestrial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>jumping</td>\n",
              "      <td>jumping</td>\n",
              "      <td>jumping</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>best</td>\n",
              "      <td>best</td>\n",
              "      <td>well</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sleeping</td>\n",
              "      <td>sleep</td>\n",
              "      <td>sleep</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a58aa76-480c-4ab2-87b5-ad73fad653cc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a58aa76-480c-4ab2-87b5-ad73fad653cc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a58aa76-480c-4ab2-87b5-ad73fad653cc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_e14a2a30-29ad-432d-b388-31d970d771d6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_lemmas')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e14a2a30-29ad-432d-b388-31d970d771d6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_lemmas');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_lemmas",
              "summary": "{\n  \"name\": \"df_lemmas\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Original word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"is\",\n          \"jumping\",\n          \"walking\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lemmatised word - WordNetLemmatizer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"be\",\n          \"jumping\",\n          \"walk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lemmatised word - spaCy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"be\",\n          \"jumping\",\n          \"walk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBkMybM0JTpQ"
      },
      "source": [
        "### NLTK WordNetLemmatizer with Part-of-Speech (PoS) tags\n",
        "\n",
        "[Parts of speech](https://www.englishclub.com/grammar/parts-of-speech.htm) are also known as word classes or lexical categories.\n",
        "\n",
        "By feeding the corresponding PoS tag along with the word, we can further improve the WordNetLemmatizer.\n",
        "\n",
        "According the NLTK's [documentation](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.WordNetLemmatizer.lemmatize), “n” for nouns, “v” for verbs, “a” for adjectives and “r” for adverbs are the valid PoS tag options for WordNetLemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')  # NLTK module required for WordNetLemmatizer\n",
        "nltk.download('omw-1.4') # NLTK module required for WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')  #  NLTK module requried for PoS tagger\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqQrXogv0KOR",
        "outputId": "8614c080-ffb6-42e5-d3a2-0f7611f29f6a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTrh7VoHJUdo",
        "outputId": "26ad82f0-a55a-4ac4-e471-e28c71333741"
      },
      "source": [
        "lemma_word=wnl.lemmatize('ponies', pos='n')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('walking', pos='v')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('better', pos='a')\n",
        "print(lemma_word)\n",
        "\n",
        "lemma_word=wnl.lemmatize('effectively', pos='r')\n",
        "print(lemma_word)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pony\n",
            "walk\n",
            "good\n",
            "effectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the PoSTagger available with [NLTK](https://www.nltk.org/api/nltk.tag.html) to automatically identify the PoS tags."
      ],
      "metadata": {
        "id": "1piMEHlZnY3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(['ponies', 'walking', 'best', 'effectively'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFzd_6iSj_bR",
        "outputId": "563a1088-b882-47e7-a42b-78351a968f75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ponies', 'NNS'), ('walking', 'VBG'), ('best', 'JJS'), ('effectively', 'RB')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNetLemmatizer requires PoS tags in the format of 'n', 'v', 'a' and 'r'.\n",
        "But, PoSTagger return tags in the format of 'NNS', 'VBG', 'JJS' and 'RB'.\n",
        "\n",
        "Let's write a simple function to get the PoS tag of a word in the format required by WordNetLemmatizer."
      ],
      "metadata": {
        "id": "MCnQP-j41MJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "1TsatnICrt8c"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are not familiar with what happens with Python dictionary get() method, find more details [here](https://www.w3schools.com/python/ref_dictionary_get.asp)."
      ],
      "metadata": {
        "id": "OAZi_OpamYiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = ['ponies', 'walking', 'best', 'effectively']\n",
        "\n",
        "for word in sample_words:\n",
        "  pos = get_wordnet_pos(word)\n",
        "  lemma_word=wnl.lemmatize(word, pos)\n",
        "  print(lemma_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0LihATls0So",
        "outputId": "69af65da-5fde-4a38-c808-d72cf955fd5f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pony\n",
            "walk\n",
            "best\n",
            "effectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJ6XKBKp-UM"
      },
      "source": [
        "# Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co78O5UX4lQa",
        "outputId": "39758abf-df79-45a5-ca20-045ae8270b6f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You do not have to create a stopword list from scratch, as NLTK provides us with a readily available list. But you may need to update us depending on the problem you try to solve."
      ],
      "metadata": {
        "id": "5qhj0M_g46Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.remove(\"off\")\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNJlwU3W5Zqa",
        "outputId": "e9c6d4fb-4a38-4736-f504-70997f6ccbf9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'do', 'has', 'you', \"she'll\", 'having', \"we've\", 'by', 'him', \"they'd\", \"hadn't\", 're', 'those', 't', 'does', 'your', 'just', 'they', 'couldn', 'below', 'for', 'herself', 'most', \"mustn't\", 'against', \"i've\", 'here', \"he's\", 'my', 'our', \"we'd\", 'at', 'but', 'his', \"shan't\", 'themselves', 'yourselves', 'will', 'too', \"should've\", 'a', \"don't\", 'doesn', \"you're\", 'now', 'more', \"weren't\", 'its', 'wouldn', 'some', \"isn't\", \"that'll\", \"won't\", 'ours', \"i'd\", 'or', 'won', 'yours', 'me', 'who', 'and', 'aren', 'been', \"haven't\", 'isn', 'on', 'theirs', 'with', 'only', 'very', 'into', \"it's\", \"i'm\", 'no', 'own', 'them', 'both', 'all', \"couldn't\", 'when', \"wouldn't\", 'we', 'ourselves', \"wasn't\", 'she', \"they're\", 'd', \"we're\", 'i', 'don', \"it'll\", 'again', 'after', \"shouldn't\", 'it', 'not', 'hasn', 'how', 'wasn', 'if', 'this', 'than', 'did', 'm', 've', \"we'll\", 'her', 'the', 'further', 'once', 'weren', 'about', 'haven', \"hasn't\", \"mightn't\", 'll', 'until', \"you've\", 'can', 'other', 'shan', 'which', 'y', 'hadn', \"he'd\", 'doing', 'over', 'shouldn', 'were', 'ma', 'each', 'of', 'was', 'mightn', 'through', 'to', 'up', 'between', 'there', 'because', \"you'll\", 'should', 'above', 'ain', 'an', 'o', 'these', \"doesn't\", \"it'd\", 'such', 'from', 'under', 'itself', 'where', 'had', 'so', 'out', 'whom', 's', \"didn't\", 'have', 'down', 'needn', 'are', 'while', \"she'd\", 'then', 'myself', \"they'll\", 'any', 'himself', 'what', \"you'd\", \"i'll\", 'in', 'be', 'as', 'same', 'that', \"he'll\", 'few', 'is', 'why', 'didn', 'mustn', 'yourself', \"she's\", 'nor', 'during', 'he', 'am', 'being', 'their', \"they've\", 'before', \"needn't\", \"aren't\", 'hers'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to remove stopwords in a sentence."
      ],
      "metadata": {
        "id": "7KBEDG3i5sBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.parse.chart import FilteredSingleEdgeFundamentalRule\n",
        "sample_text = \"This is a sample sentence, showing off the stop words removal.\"\n",
        "\n",
        "# tokenise text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# remove stopwords from tokens\n",
        "filtered_words = [token for token in tokens if token not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "id": "wrCkf5fg8cVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6704657-9953-4f64-f5e6-4b00e0ef2c71"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'sample', 'sentence', ',', 'showing', 'off', 'stop', 'words', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'>**Activity 3**</font>\n",
        "\n",
        "Update the default stop word list by removing 'off', and remove stop words in the sample_text.\n",
        "\n",
        "**Expected output:** \\['This', 'sample', 'sentence', ',', 'showing', 'off', 'stop', 'words', 'removal', '.']\n",
        "\n",
        "**Hint:** [Python - Remove List Items](https://www.w3schools.com/python/python_lists_remove.asp)"
      ],
      "metadata": {
        "id": "Pz8L00n9J6gV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OParvOihqaK4"
      },
      "source": [
        "# Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "XmcF_eE86-Xk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get a readily available set of punctuations using the Python string package."
      ],
      "metadata": {
        "id": "ihXs9OLs7Egi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Punctuation marks: {string.punctuation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeBf46Jv7aKc",
        "outputId": "f7138240-44bc-4453-8d4d-9bfa3077547c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation marks: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Let's remove punctuation marks!\"\n",
        "\n",
        "# remove puncuation marks in sample text\n",
        "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "no_punctuation= sample_text.translate(table)\n",
        "\n",
        "print(no_punctuation)"
      ],
      "metadata": {
        "id": "IocTzFuv-Q0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcbce0e-b340-49b8-9e38-d128f0724383"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lets remove punctuation marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "Let's see how to use [spaCy](https://spacy.io/usage/linguistic-features#named-entities) models for NER.\n",
        "\n",
        "[spaCy English Models](https://spacy.io/models/en)"
      ],
      "metadata": {
        "id": "hNuUElQSzvm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm  # spacy model\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "YmZH1stw8S1g"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Apple is looking at buying U.K. startup for $1 billion\""
      ],
      "metadata": {
        "id": "Mq0ULRvWzuyF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "izGIa-ai1cI-",
        "outputId": "488510c1-4033-4995-b4e8-6e785694cdac"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace text with recognised named entities."
      ],
      "metadata": {
        "id": "rEmztv4PGTSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sample_text)\n",
        "\n",
        "updated_tokens = [t.text if not t.ent_type_ else t.ent_type_ for t in doc]\n",
        "updated_sentence = \" \".join(updated_tokens)\n",
        "print(updated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwYS5IUhGaij",
        "outputId": "871686e8-4c18-4571-9eba-664bedb8b7c9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG is looking at buying GPE startup for MONEY MONEY MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repitetions of the same named entity can be merged by adding ['merge_entities'](https://spacy.io/api/pipeline-functions#merge_entities) to the pipeline."
      ],
      "metadata": {
        "id": "tlpTMJD-H3EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "updated_tokens = [t.text if not t.ent_type_ else t.ent_type_ for t in doc]\n",
        "updated_sentence = \" \".join(updated_tokens)\n",
        "print(updated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQjhekyzH1XE",
        "outputId": "a7172abf-12e2-41a1-c68c-e36df52cb4c0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG is looking at buying GPE startup for MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='green'>**Activity 4**</font>\n",
        "\n",
        "Let's assume you need to identify the sentiment (positive, negative and neutral) of a given product review. A few sample reviews are given bellow.\n",
        "\n",
        "* \"Apple's new product is amazing.\"\n",
        "* \"I'm quite dissapointed with recent Apple products.\"\n",
        "* \"Android products are amazing and versatile.\"\n",
        "\n",
        "a) Replace the entities in these sentences using entity tags.\n",
        "\n",
        "b) Would this replacement be helpful for sentiment identification from the perpective of a machine learning model?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VYUfy6jy8qrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Apple's new product is amazing.\"\n",
        "doc = nlp(sample_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "IOl0BUN70mYD",
        "outputId": "912eccc9-41b2-443d-ac14-857dd43d85ff"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 0 5 ORG\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s new product is amazing.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"I'm quite dissapointed with recent Apple products.\"\n",
        "doc = nlp(sample_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXBciucf0v2P",
        "outputId": "9a2ba901-c93c-4dd8-f71a-97bbd076ff5e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 35 40 ORG\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I'm quite dissapointed with recent \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " products.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Android products are amazing and versatile.\"\n",
        "doc = nlp(sample_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bRfdXhTn057A",
        "outputId": "80327d71-573d-45db-8f4c-273dbe614181"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Android 0 7 ORG\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Android\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " products are amazing and versatile.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e564803"
      },
      "source": [
        "# Task\n",
        "Replace named entities in the provided sample product reviews (\"Apple's new product is amazing.\", \"I'm quite dissapointed with recent Apple products.\", \"Android products are amazing and versatile.\") with their corresponding entity tags using spaCy, display the original and entity-replaced reviews, and then discuss whether this entity replacement would be beneficial for a machine learning model performing sentiment identification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a17d078"
      },
      "source": [
        "## Define Sample Reviews\n",
        "\n",
        "### Subtask:\n",
        "Store the provided sample product review sentences in a Python list for processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65789b9f"
      },
      "source": [
        "**Reasoning**:\n",
        "To store the sample product review sentences as requested, I will create a Python list named `sample_reviews` and populate it with the provided strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb015e8d",
        "outputId": "cef838da-e465-4815-e0f7-ebbbb24b5d6d"
      },
      "source": [
        "sample_reviews = [\n",
        "    \"Apple's new product is amazing.\",\n",
        "    \"I'm quite dissapointed with recent Apple products.\",\n",
        "    \"Android products are amazing and versatile.\"\n",
        "]\n",
        "\n",
        "print(\"Sample reviews list created successfully.\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample reviews list created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60fed9d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the sample reviews are stored, I will process each review using spaCy to identify and replace named entities with their corresponding entity tags. This will fulfill the first part of the task which is to replace the entities in these sentences using entity tags and display the original and entity-replaced reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "9535f6b5",
        "outputId": "cfb85e0f-210c-4269-b4ed-4dff315aa420"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "processed_reviews = []\n",
        "\n",
        "for review in sample_reviews:\n",
        "    doc = nlp(review)\n",
        "    # Get original entities and their labels for display\n",
        "    entities_found = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Generate entity-replaced sentence\n",
        "    # First, ensure 'merge_entities' is in the pipeline if not already. This was added in cell iQjhekyzH1XE.\n",
        "    # However, for correct replacement, it's often better to iterate over tokens and replace if they are part of an entity.\n",
        "    # Let's re-run the pipeline with `merge_entities` for each doc to be safe if the pipeline was reset or modified.\n",
        "\n",
        "    # Create a new nlp object to ensure merge_entities is added for this specific task scope,\n",
        "    # or verify it's persistently added to the existing 'nlp' object.\n",
        "    # Based on previous cells, 'nlp.add_pipe(\"merge_entities\")' was run once, so it should be active.\n",
        "\n",
        "    # Re-process the document with potentially merged entities for consistent replacement logic.\n",
        "    # A safer approach for displaying original entities and then replaced ones separately without modifying the main nlp pipeline is as follows:\n",
        "\n",
        "    # Create a temporary nlp object with merge_entities for generating the replaced sentence\n",
        "    temp_nlp = en_core_web_sm.load()\n",
        "    temp_nlp.add_pipe(\"merge_entities\")\n",
        "    temp_doc = temp_nlp(review)\n",
        "\n",
        "    updated_tokens = [token.text if not token.ent_type_ else token.ent_type_ for token in temp_doc]\n",
        "    replaced_sentence = \" \".join(updated_tokens)\n",
        "\n",
        "    processed_reviews.append({\n",
        "        'Original Review': review,\n",
        "        'Entities Found': entities_found,\n",
        "        'Entity-Replaced Review': replaced_sentence\n",
        "    })\n",
        "\n",
        "df_processed_reviews = pd.DataFrame(processed_reviews)\n",
        "display(df_processed_reviews)\n",
        "\n",
        "print(\"Reviews processed and entities replaced.\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                     Original Review    Entities Found  \\\n",
              "0                    Apple's new product is amazing.    [(Apple, ORG)]   \n",
              "1  I'm quite dissapointed with recent Apple produ...    [(Apple, ORG)]   \n",
              "2        Android products are amazing and versatile.  [(Android, ORG)]   \n",
              "\n",
              "                              Entity-Replaced Review  \n",
              "0                    ORG 's new product is amazing .  \n",
              "1  I 'm quite dissapointed with recent ORG produc...  \n",
              "2           ORG products are amazing and versatile .  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a0847e5-4611-4429-b24c-ab6b553844e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Review</th>\n",
              "      <th>Entities Found</th>\n",
              "      <th>Entity-Replaced Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Apple's new product is amazing.</td>\n",
              "      <td>[(Apple, ORG)]</td>\n",
              "      <td>ORG 's new product is amazing .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm quite dissapointed with recent Apple produ...</td>\n",
              "      <td>[(Apple, ORG)]</td>\n",
              "      <td>I 'm quite dissapointed with recent ORG produc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Android products are amazing and versatile.</td>\n",
              "      <td>[(Android, ORG)]</td>\n",
              "      <td>ORG products are amazing and versatile .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a0847e5-4611-4429-b24c-ab6b553844e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a0847e5-4611-4429-b24c-ab6b553844e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a0847e5-4611-4429-b24c-ab6b553844e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_271daa0c-79bf-4b97-bf3c-6de6b76fed63\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_processed_reviews')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_271daa0c-79bf-4b97-bf3c-6de6b76fed63 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_processed_reviews');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_processed_reviews",
              "summary": "{\n  \"name\": \"df_processed_reviews\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Original Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Apple's new product is amazing.\",\n          \"I'm quite dissapointed with recent Apple products.\",\n          \"Android products are amazing and versatile.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Entities Found\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Entity-Replaced Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ORG 's new product is amazing .\",\n          \"I 'm quite dissapointed with recent ORG products .\",\n          \"ORG products are amazing and versatile .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews processed and entities replaced.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6e61974"
      },
      "source": [
        "### b) Would this replacement be helpful for sentiment identification from the perspective of a machine learning model?\n",
        "\n",
        "**Discussion:**\n",
        "\n",
        "Replacing specific named entities (like product names or company names) with their generic entity tags (e.g., 'ORG' for Organization) can be a helpful preprocessing step for sentiment identification, but its utility depends on the specific goals and characteristics of the machine learning model:\n",
        "\n",
        "**Potential Benefits:**\n",
        "\n",
        "1.  **Generalization:** By replacing specific entities with a generic tag, the model can learn sentiment patterns that are independent of the particular entity. For example, if 'Apple' is always replaced with 'ORG', the model learns that \"ORG's new product is amazing\" is positive, rather than specifically learning that \"Apple's new product is amazing\" is positive. This helps the model generalize to new, unseen organizations or products.\n",
        "2.  **Reduced Vocabulary Size:** If a dataset contains many different entities, replacing them with a few generic tags can reduce the overall vocabulary size, simplifying the model and potentially improving training efficiency, especially for models that struggle with large vocabularies.\n",
        "3.  **Focus on Core Sentiment:** This approach forces the model to focus on the words surrounding the entity that convey sentiment (e.g., 'amazing', 'disappointed', 'versatile'), rather than on the entity itself. This can be beneficial if the goal is to extract general sentiment regardless of *who* or *what* the sentiment is about.\n",
        "\n",
        "**Potential Drawbacks/Considerations:**\n",
        "\n",
        "1.  **Loss of Entity-Specific Nuance:** In some cases, the sentiment might be intrinsically tied to the entity. For example, a model might need to learn that 'Apple' is often associated with positive sentiment regarding innovation, while another 'ORG' might be associated with different sentiment patterns. Replacing them generically loses this specific context.\n",
        "2.  **Ambiguity:** If the model needs to distinguish between sentiment towards different types of entities (e.g., positive sentiment towards a 'PERSON' vs. positive sentiment towards a 'GPE'), then generic replacement might hinder this distinction.\n",
        "3.  **Domain Dependence:** The helpfulness of this technique is highly dependent on the domain. For general product reviews, it might be very useful, but for highly specialized sentiment analysis (e.g., analyzing public opinion on specific political figures), retaining the entity information might be crucial.\n",
        "\n",
        "**Conclusion for this specific scenario (product reviews):**\n",
        "\n",
        "For identifying general sentiment (positive, negative, neutral) in product reviews, replacing entities with tags like 'ORG' is likely **beneficial**. It allows the model to learn that phrases like \"ORG's new product is amazing\" and \"dissapointed with recent ORG products\" indicate positive and negative sentiment, respectively, regardless of whether the organization is Apple, Android, or another company. This improves the model's ability to generalize across different product manufacturers and focuses its learning on the sentiment-bearing words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82a54c0d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, replacing named entities with their corresponding entity tags (e.g., 'ORG' for Organization) in product reviews is likely beneficial for a machine learning model performing sentiment identification. This approach enhances the model's ability to generalize sentiment patterns beyond specific brand names, focusing its learning on the sentiment-bearing words.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The provided sample product reviews were successfully stored for processing.\n",
        "*   spaCy was utilized to identify and replace named entities in the reviews. For instance, \"Apple\" and \"Android\" were recognized as `ORG` (Organization) entities.\n",
        "*   The original review \"Apple's new product is amazing.\" was transformed into \"ORG 's new product is amazing .\"\n",
        "*   Similarly, \"I'm quite dissapointed with recent Apple products.\" became \"I 'm quite dissapointed with recent ORG products .\"\n",
        "*   The review \"Android products are amazing and versatile.\" was replaced with \"ORG products are amazing and versatile .\"\n",
        "*   The process successfully generated a DataFrame displaying the original review, the entities found (e.g., `('Apple', 'ORG')`), and the entity-replaced review for each sample.\n",
        "*   The discussion highlighted that this replacement offers potential benefits such as improved generalization, reduced vocabulary size for the model, and a clearer focus on core sentiment.\n",
        "*   Potential drawbacks identified include a loss of entity-specific nuance, potential ambiguity, and domain dependence.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Replacing specific entities with generic tags significantly improves a model's ability to generalize sentiment across different brands or products, as it learns sentiment from contextual words rather than specific names.\n",
        "*   Further evaluate the impact of this entity replacement strategy on the performance of a machine learning model for sentiment analysis using a larger, diverse dataset of product reviews.\n"
      ]
    }
  ]
}